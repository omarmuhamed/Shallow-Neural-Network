{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow neural network (1-hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook I will explain the structure of a 2-layers or 1-hidden layer neural network. We will be building the neural network from scratch by using plain python, NumPy and Matplotlib for visualization. The model predicts the color of points, label (y=0) for red and label (y=1) for blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing required libraries\n",
    "- **NumPy** is used to do mathematical operations on arrays as it contains a multi-dimensional array and matrix data structures.\n",
    "-  **Matplotlib** is a plotting library used for visualizing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating the dataset that will be used in training the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  points with an x1 component or a x2 component, exclusively, above 0.5 have a classification value of 1, otherwise (if both x1 and x2 components are greater than or less than 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(X1, X2):\n",
    "        return 0 if (X1 > .5 and X2 >.5) or (X1 < .5 and X2 < .5) else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function generates and returns our dataset. It takes one argument called 'size' which detirmines the number of samples or observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It returns an array of our input which in this example will be (2, size) dimensional matrix, and a (1, size) dimensional array of classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates dataset\n",
    "    \n",
    "    Arguments:\n",
    "    size -- size if the dataset\n",
    "    \n",
    "    Returns:\n",
    "    X -- Input data in shape of (2, size)\n",
    "    Y -- Output data in shape of (1, size)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Create an array of the given size and fill it with random values between 0 and 1\n",
    "    X = np.random.rand(2, size)\n",
    "    \n",
    "    # Create a list of tuples of each pair of the array created above\n",
    "    features = list(zip(X[0], X[1]))\n",
    "    \n",
    "    # Create an array of classification labels\n",
    "    Y = np.array([xor(x1 ,x2) for x1, x2 in features])\n",
    "    \n",
    "    # Reshape our labels to (1, size) dimensional array\n",
    "    Y.shape = (1, size)\n",
    "\n",
    "    # Return the input and output layers\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation functions are mathematical equations that determine the output of a neural network by making the incoming data non-linear so that the network can learn complex patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Types of activation functions: Sigmoid, tanh, ReLU, leaky ReLU,  etc.. \n",
    "- I will be using only 2 functions as we have 2 layers in our network:\n",
    "\n",
    "    - tanh: which is a built-in function in numpy and its fomula as follows:\n",
    "    \n",
    "        **$$\\tanh{x} = \\left(\\frac{2}{1 + {exp}^{-2}} - 1\\right)$$**\n",
    "\n",
    "    - sigmoid: \n",
    "        \n",
    "        $$\\sigma = \\frac{1}{1 + {exp}^{-x}}$$\n",
    "        \n",
    "        \n",
    "- **I will make a comparison notebook explaining the activation functions in depth later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # np.exp() calculates the exponential of all elements in the input array.\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initializing model's parameters\n",
    "   - Initialization step is critical to model's overall performance and should be done carefully.\n",
    "   - The weights should be initialized randomly as if we initialized them to zero will lead the nodes to calculate the same features which prevents different neurons from learning different things. So we have to break the symmetry by randomly initializing them.\n",
    "   - The bias can be initialized to zero unlike the weights\n",
    "   \n",
    "**Parameters initialization will be explained in depth later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(hidden_nodes, features, output_layers):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializing the parameters \n",
    "    \n",
    "    Arguments:\n",
    "    hidden_layers -- number of hidden nodes in the hidden layer\n",
    "    features -- number of features in our dataset\n",
    "    output_layers -- number of outputs \n",
    "    \n",
    "    Returns:\n",
    "    W -- randomly initialized weights\n",
    "    B -- Bias initialized to zero\n",
    "    \"\"\"\n",
    "    # Initializing the weights with random values\n",
    "    \n",
    "    #First layer's weights is (hidden_node, features) dimensional array as it takes the features as input\n",
    "    W1 = np.random.randn(hidden_nodes, features) * 0.01\n",
    "    \n",
    "    #Second layer's weights (the output layer) is (output_layers, hidden_nodes) dimensional array as it takes the hidden layer's output as input\n",
    "    W2 = np.random.randn(output_layers, hidden_nodes) * 0.01\n",
    "    \n",
    "    # Initializing bias with zero\n",
    "    b1 = np.zeros((hidden_nodes, 1))\n",
    "    b2 = np.zeros((output_layers, 1))\n",
    "    \n",
    "    # Adding the parameters to dictionary\n",
    "    W = {'W1': W1, 'W2': W2}\n",
    "    B = {'b1': b1, 'b2': b2}\n",
    "    \n",
    "    # Return the parameters\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Forward propagation\n",
    "- Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer. \n",
    "- Each hidden layer accepts the input data, processes it as per the activation function and passes to the successive layer. We now work step-by-step through the mechanics of a neural network with one hidden layer.\n",
    "\n",
    "\n",
    "**$$Z^{[1]}=W^{[1]}.X + b^{[1]}$$**\n",
    "\n",
    "**$$A^{[1]}=\\tanh(Z^{[1]})$$**\n",
    "\n",
    "**$$Z^{[2]}=W^{[2]}.A^{[1]} + b^{[2]}$$**\n",
    "\n",
    "**$$A^{[2]}=\\sigma(Z^{[2]})$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W, B, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applying forward propagation using the equations given above\n",
    "    \n",
    "    Arguments:\n",
    "    W -- The weights of our layers\n",
    "    B -- The bias\n",
    "    X -- Input data in shape of (2, samples)\n",
    "    \n",
    "    Returns:\n",
    "    A1 -- Activations of the hidden layer\n",
    "    A2 -- Activations of the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = W['W1']\n",
    "    W2 = W['W2']\n",
    "    \n",
    "    b1 = B['b1']\n",
    "    b2 = B['b2']\n",
    "    \n",
    "    # Calculating Z of the hidden layer using np.dot() which does matrix dot production\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    \n",
    "    # Calculating activations of the hidden layer using np.tanh which applies the tanh function to the whole matrix elements\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    # Calculating Z of the output layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    # Calculating activations of the output layer using sigmoid function\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # Return the activation values\n",
    "    return A1, A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cost\n",
    "- After calculating the activations now we are able to calculate the cost and what the cost function really doing is calculating the overall performance of our model by quantifing the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "\n",
    "<img src=\"images/CostEquation.gif\" />\n",
    "\n",
    "- For the log function we can use numpy builtin log function  np.log()\n",
    "- np.squeeze() is being used to remove redundant dimensions\n",
    "- Y.T is the transpose of Y matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y, A):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the cost using the equation given above\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- Output data in shape of (1, samples)\n",
    "    A -- Activations of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost using the equation given above\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate output count\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Calculating the cost using the equation given above\n",
    "    cost = (-1 / m) * np.sum((np.dot(np.log(A), Y.T) + np.dot(np.log(1 - A), (1 - Y).T)))\n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Backward propagation\n",
    "- Backpropagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector.\n",
    "- Backpropagation aims to minimize the cost function by adjusting networkâ€™s weights and biases and the level of adjustment is determined by the gradients of the cost function with respect to those parameters.\n",
    "- Calculating the gradient tells us how much should the weight and bias change in order to minimize the cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$$**\n",
    "\n",
    "**$$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $$**\n",
    "\n",
    "**$$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$$**\n",
    "\n",
    "**$$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$**\n",
    "\n",
    "**$$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $$**\n",
    "\n",
    "**$$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$$**\n",
    "\n",
    "   - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "   - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "   - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "   - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(W, A, X, Y):\n",
    "    \"\"\"\n",
    "    Applying backward propagation using the equations above\n",
    "    \n",
    "    Arguments:\n",
    "    W -- Python dictionary contains the weights of our network\n",
    "    A -- Python dictionary contains the activiation values of the layers\n",
    "    X -- Input data in shape of (2, samples)\n",
    "    Y -- Output data in shape of (1, samples)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- Python dictionary contains our gradiants\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Samples count\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Getting the weights from W\n",
    "    W1 = W['W1']\n",
    "    W2 = W['W2']\n",
    "    \n",
    "    #Getting the activations from B\n",
    "    A1 = A['A1']\n",
    "    A2 = A['A2']\n",
    "    \n",
    "    #Calculating activations using equations given above\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {'dW2': dW2,'db2': db2, 'dW1': dW1, 'db1': db1}\n",
    "    \n",
    "    return gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Updating the parameters\n",
    "\n",
    "- After finishing the backward propagation process and getting the gradients we are ready to update our parameters using the general gradient descent rule **$$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$$** where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W, B, gradients, learning_rate = 1.2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Updates the parameters using gradient descent rule\n",
    "    \n",
    "    Arguments:\n",
    "    W -- The weights\n",
    "    B -- The bias\n",
    "    gradients -- The calculated gradients from backward propagation process\n",
    "    learning_rate -- the step size at each iteration\n",
    "    \n",
    "    Returns:\n",
    "    W -- The updated weights\n",
    "    B -- The updated bias\n",
    "    \"\"\"\n",
    "    \n",
    "    #Getting the parameters\n",
    "    W1 = W['W1']\n",
    "    W2 = W['W2']\n",
    "    \n",
    "    b1 = B['b1']\n",
    "    b2 = B['b2']\n",
    "    \n",
    "    # Getting the gradients\n",
    "    dW1 = gradients['dW1']\n",
    "    dW2 = gradients['dW2']\n",
    "    \n",
    "    db1 = gradients['db1']\n",
    "    db2 = gradients['db2']\n",
    "    \n",
    "    #Calculating the new parameters using gradient descent rule\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    \n",
    "    b1 -= learning_rate * db1\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    W = {'W1': W1, 'W2': W2}\n",
    "    B = {'b1': b1, 'b2': b2}\n",
    "    \n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to collect everything together and build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, hidden_nodes, iterations = 20000):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains our neural network\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input data in shape of (2, samples)\n",
    "    Y -- Output data in shape of (1, samples)\n",
    "    hidden_nodes -- count of nodes in the hidden layer\n",
    "    iterations -- Number of iterations in gradient descent loop\n",
    "    \n",
    "    Returns:\n",
    "    W -- The weights learnt by the model\n",
    "    B -- The bias learnt by the model\n",
    "    cost_list -- list contains the cost of every iteration\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cost_list = []\n",
    "    \n",
    "    # Initializing our parameters\n",
    "    W, B = initialize_parameters(hidden_nodes, X.shape[0], Y.shape[0])\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Calculating the activations\n",
    "        A1, A2 = forward_propagation(W, B, X)\n",
    "        \n",
    "        # Calculating the cost and appending it in the cost_list\n",
    "        cost_ = cost(Y, A2)\n",
    "        cost_list.append(cost_)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        gradients = backward_propagation(W, {'A1': A1, 'A2': A2}, X, Y)\n",
    "        \n",
    "        #Updating parameters\n",
    "        W, B = update_parameters(W, B, gradients)\n",
    "        \n",
    "        # Print the cost every 1000 iteration \n",
    "        if i % 1000 == 0:\n",
    "            print('Cost after %i iterations = %f'%(i, cost_))\n",
    "            \n",
    "    return W, B, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, B, X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts the label of each example\n",
    "    \n",
    "    Arguments:\n",
    "    W -- Weights\n",
    "    B -- Bias\n",
    "    X -- Input Data\n",
    "    \n",
    "    Returns:\n",
    "    predictions: the predicted labels\n",
    "    \"\"\"\n",
    "    \n",
    "    #Calculating activations\n",
    "    A1, A2 = forward_propagation(W, B, X)\n",
    "    \n",
    "    #For each activation in the output layer higher than 0.5 the prediction is true otherwise the prediction is false\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset with 500 sample \n",
    "X, Y = generate_dataset(500)\n",
    "\n",
    "# Build a model with 5 hidden nodes\n",
    "iterations = 10000\n",
    "W, B, costs = model(X, Y, 5, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the accuracy\n",
    "predictions = predict(W, B, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the decision boundary\n",
    "xx, yy = np.meshgrid(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "t = lambda x: predict(W, B, x.T)\n",
    "Z = t(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "plt.ylabel('x2')\n",
    "plt.xlabel('x1')\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the cost function with the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(iterations)), costs, '-r')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
